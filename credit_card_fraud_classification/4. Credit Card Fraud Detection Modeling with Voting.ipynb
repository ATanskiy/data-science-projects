{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63774133",
   "metadata": {},
   "source": [
    "# Welcome to this lovely notebook. This is the extention of the notebooks of modeling. Here we'll try to use voting classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b381e7b5",
   "metadata": {},
   "source": [
    "## In this notebook we are going to implement the following:\n",
    "\n",
    "1. We'll try to use voting to get better results by the use of 3 models: XGBoost, LightGBM, CatBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b9830c",
   "metadata": {},
   "source": [
    "## Importing all required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13438beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all the required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "# Set the option to display all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# metrics\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score,\\\n",
    "classification_report, precision_recall_curve, auc, make_scorer, fbeta_score\n",
    "\n",
    "# Encoding\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Machine Learning - Preparation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Machine Learning - Algorithm\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc9ff52",
   "metadata": {},
   "source": [
    "## 1 Let's get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9789eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('fraudTrain.csv', index_col=0)\n",
    "test = pd.read_csv('fraudTest.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0495bfc",
   "metadata": {},
   "source": [
    "## 2 Let's split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83427eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.drop(columns=['is_fraud'])\n",
    "y_train = train['is_fraud']\n",
    "\n",
    "X_test = test.drop(columns=['is_fraud'])\n",
    "y_test = test['is_fraud']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094aaec3",
   "metadata": {},
   "source": [
    "## 3 Preparing the 50% downsampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "221fb207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 3min 41s\n",
      "Wall time: 1min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "X_train_encoded_3 = pd.read_parquet('X_train_encoded.csv')\n",
    "X_test_encoded_3 = pd.read_parquet('X_test_encoded.csv')\n",
    "\n",
    "# Reset the indices to align them\n",
    "X_train_encoded_3 = X_train_encoded_3.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "\n",
    "# Step 1: Separate majority class (0s) and minority class (1s)\n",
    "X_train_encoded_0 = X_train_encoded_3[y_train == 0]\n",
    "X_train_encoded_1 = X_train_encoded_3[y_train == 1]\n",
    "\n",
    "y_train_0 = y_train[y_train == 0]\n",
    "y_train_1 = y_train[y_train == 1]\n",
    "\n",
    "# Downsample X_train_encoded_0 and use its indices to select the corresponding rows from y_train_0\n",
    "X_train_0_downsampled = X_train_encoded_0.sample(frac=0.5, random_state=42)\n",
    "y_train_0_downsampled = y_train_0.loc[X_train_0_downsampled.index]  # Use the same indices for y_train\n",
    "\n",
    "# Step 3: Concatenate the downsampled majority class with the minority class\n",
    "X_train_downsampled = pd.concat([X_train_0_downsampled, X_train_encoded_1])\n",
    "y_train_downsampled = pd.concat([y_train_0_downsampled, y_train_1])\n",
    "\n",
    "# Step 4: Shuffle the dataset to mix the downsampled rows\n",
    "X_train_encoded_3 = X_train_downsampled.sample(frac=1, random_state=42)\n",
    "y_train = y_train_downsampled.loc[X_train_downsampled.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "011ad9ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(652090, 2151)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_encoded_3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f087b1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "test\n",
      "X_train\n",
      "X_test\n",
      "X_train_encoded_3\n",
      "X_test_encoded_3\n",
      "X_train_encoded_0\n",
      "X_train_encoded_1\n",
      "X_train_0_downsampled\n",
      "X_train_downsampled\n"
     ]
    }
   ],
   "source": [
    "# List all DataFrames in memory\n",
    "dfs_in_memory = {name: obj for name, obj in globals().items() if isinstance(obj, pd.DataFrame)}\n",
    "\n",
    "# Display the DataFrame names\n",
    "for name in dfs_in_memory:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd53c593",
   "metadata": {},
   "source": [
    "### Removing unsed dfs from memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cf140e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "del train, test, X_train, X_test, X_train_encoded_0, X_train_encoded_1, X_train_0_downsampled, X_train_downsampled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba126686",
   "metadata": {},
   "source": [
    "### Define the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f96a992c",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(max_depth=7, learning_rate=0.05, sub_sample=0.9, scale_pos_weight=0.8)\n",
    "lgbm = LGBMClassifier(max_depth=7, learning_rate=0.05, n_estimators=100, scale_pos_weight=0.8)\n",
    "catboost = CatBoostClassifier(verbose=0, depth=7, learning_rate=0.2, n_estimators=100, scale_pos_weight=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758b9c83",
   "metadata": {},
   "source": [
    "### Define the classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3902cb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_clf_hard = VotingClassifier(estimators=[\n",
    "    ('xgb', xgb), \n",
    "    ('lgbm', lgbm), \n",
    "    ('catboost', catboost)],\n",
    "    voting='hard'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19370c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_clf_soft = VotingClassifier(estimators=[\n",
    "    ('xgb', xgb), \n",
    "    ('lgbm', lgbm), \n",
    "    ('catboost', catboost)],\n",
    "    voting='soft'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b76cc55b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\atans\\AppData\\Roaming\\Python\\Python311\\site-packages\\xgboost\\core.py:160: UserWarning: [16:31:15] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0750514818a16474a-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:742: \n",
      "Parameters: { \"sub_sample\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 7506, number of negative: 644584\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004017 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4748\n",
      "[LightGBM] [Info] Number of data points in the train set: 652090, number of used features: 2084\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011511 -> initscore=-4.452902\n",
      "[LightGBM] [Info] Start training from score -4.452902\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "\n",
      "\n",
      "This is a set of results for 50% downsample\n",
      "\n",
      "\n",
      "Classification Report for Training Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    644584\n",
      "           1       0.89      0.69      0.78      7506\n",
      "\n",
      "    accuracy                           1.00    652090\n",
      "   macro avg       0.94      0.84      0.89    652090\n",
      "weighted avg       1.00      1.00      1.00    652090\n",
      "\n",
      "\n",
      "Classification Report for Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    553574\n",
      "           1       0.71      0.66      0.68      2145\n",
      "\n",
      "    accuracy                           1.00    555719\n",
      "   macro avg       0.85      0.83      0.84    555719\n",
      "weighted avg       1.00      1.00      1.00    555719\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\atans\\AppData\\Roaming\\Python\\Python311\\site-packages\\xgboost\\core.py:160: UserWarning: [16:34:40] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0750514818a16474a-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:742: \n",
      "Parameters: { \"sub_sample\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 7506, number of negative: 644584\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003626 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4748\n",
      "[LightGBM] [Info] Number of data points in the train set: 652090, number of used features: 2084\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011511 -> initscore=-4.452902\n",
      "[LightGBM] [Info] Start training from score -4.452902\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "\n",
      "\n",
      "This is a set of results for 50% downsample\n",
      "\n",
      "\n",
      "Classification Report for Training Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    644584\n",
      "           1       0.88      0.69      0.77      7506\n",
      "\n",
      "    accuracy                           1.00    652090\n",
      "   macro avg       0.94      0.84      0.89    652090\n",
      "weighted avg       1.00      1.00      1.00    652090\n",
      "\n",
      "\n",
      "Classification Report for Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    553574\n",
      "           1       0.71      0.66      0.68      2145\n",
      "\n",
      "    accuracy                           1.00    555719\n",
      "   macro avg       0.85      0.83      0.84    555719\n",
      "weighted avg       1.00      1.00      1.00    555719\n",
      "\n",
      "CPU times: total: 56min 59s\n",
      "Wall time: 8min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "X_train_encoded = pd.read_parquet('X_train_encoded.csv')\n",
    "X_test_encoded = pd.read_parquet('X_test_encoded.csv')\n",
    "\n",
    "# Reset the indices to align them\n",
    "X_train_encoded = X_train_encoded.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "\n",
    "# Step 1: Separate majority class (0s) and minority class (1s)\n",
    "X_train_encoded_0 = X_train_encoded[y_train == 0]\n",
    "X_train_encoded_1 = X_train_encoded[y_train == 1]\n",
    "\n",
    "y_train_0 = y_train[y_train == 0]\n",
    "y_train_1 = y_train[y_train == 1]\n",
    "\n",
    "\n",
    "for model in [voting_clf_hard, voting_clf_soft]:\n",
    "\n",
    "    # Step 2: Downsample the majority class (0s) by frac\n",
    "    # Downsample X_train_encoded_0 and use its indices to select the corresponding rows from y_train_0\n",
    "    X_train_0_downsampled = X_train_encoded_0.sample(frac=0.5, random_state=42)\n",
    "    y_train_0_downsampled = y_train_0.loc[X_train_0_downsampled.index]  # Use the same indices for y_train\n",
    "\n",
    "    # Step 3: Concatenate the downsampled majority class with the minority class\n",
    "    X_train_downsampled = pd.concat([X_train_0_downsampled, X_train_encoded_1])\n",
    "    y_train_downsampled = pd.concat([y_train_0_downsampled, y_train_1])\n",
    "\n",
    "    # Step 4: Shuffle the dataset to mix the downsampled rows\n",
    "    X_train_downsampled_3 = X_train_downsampled.sample(frac=1, random_state=42)\n",
    "    y_train_downsampled_3 = y_train_downsampled.loc[X_train_downsampled_3.index]  # Align y_train after shuffling\n",
    "\n",
    "    # Step 5: Initialize voting classifier\n",
    "    model_3 = model\n",
    "    \n",
    "    # Step 6: Train the model on the training data\n",
    "    model_3.fit(X_train_downsampled_3, y_train_downsampled_3)\n",
    "\n",
    "    # Step 7: Predict on the training data\n",
    "    y_train_pred = model_3.predict(X_train_downsampled_3)\n",
    "\n",
    "    # Step 8: Predict on the test data\n",
    "    y_test_pred = model_3.predict(X_test_encoded)\n",
    "\n",
    "    print('\\n')\n",
    "    print(f'This is a set of results for 50% downsample')\n",
    "    print('\\n')\n",
    "\n",
    "    # Step 9: Generate the classification report for training data\n",
    "    print(\"Classification Report for Training Data:\")\n",
    "    print(classification_report(y_train_downsampled_3, y_train_pred))\n",
    "\n",
    "    # Step 10: Generate the classification report for test data\n",
    "    print(\"\\nClassification Report for Test Data:\")\n",
    "    print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63c5b31",
   "metadata": {},
   "source": [
    "#### Conclustions\n",
    "1. The best results of voting classifier are the same as the results of LightGBM. They are quite balanced: 66% recall and 71% precision\n",
    "2. Looks like LightGBM did the best balanced job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d951d70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
