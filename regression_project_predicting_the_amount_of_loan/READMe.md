# ğŸ¦ Loan Amount Prediction â€“ Regression Project

This project tackles a regression problem involving **loan sanction amount prediction** based on applicant information such as income, expenses, credit score, employment status, and property type. The data cleaning, preprocessing, feature engineering, and model comparisons are all done to optimize prediction performance.


---

## ğŸ“ Project Structure

```
ğŸ“‚ Loan_Amount_Regression/
â”œâ”€â”€ Regression Project Aleksander Tanskii.ipynb   # Full analysis and modeling
â”œâ”€â”€ train.csv                                     # Dataset (from Kaggle)
â”œâ”€â”€ Regression Project Loan.pptx                  # Final presentation
â””â”€â”€ README.md                                     # Project summary
```

---

## ğŸ” Problem Statement

Predict the **Loan Sanction Amount (USD)** based on a dataset of loan applicants, originally designed for a HackerEarth ML competition.

---

## ğŸ§¾ Dataset Overview

- ğŸ“‚ Source: [Kaggle Dataset](https://www.kaggle.com/datasets/phileinsophos/predict-loan-amount-data/data)
- ğŸ“Š Size: 30,000 rows, 24 columns
- ğŸ¯ Target variable: `Loan Sanction Amount (USD)`
- ğŸ·ï¸ Features: Age, Income, Expenses, Credit Score, Profession, Income Stability, Property Location, etc.
- âš ï¸ Missing and anomalous values (e.g., -999) were found and handled during EDA

---

## ğŸ§¹ EDA and Preprocessing

- âœ… Removed outliers (loan amounts > $300K, negative expenses, age 18 anomalies)
- âœ… Imputed missing values using profession medians or property location modes
- âœ… Cleaned rows with -999 in the target column
- ğŸ§¼ Final dataset size: **28,044 records** (after ~6% cleanup)

---

## ğŸ› ï¸ Feature Engineering

- Created `age_group` column to reflect age distribution effects
- Applied **log transforms** to skewed numerical variables
- Scaled numerical features
- Used `get_dummies()` for all categorical variables

---

## ğŸ¤– Models Used

### Linear Regression

| Config | RMSE Train | RMSE Test |
|--------|-------------|-----------|
| All features, scaling/logs | 29,361 | 29,914 |
| Numerical only, no logs | 27,216 | 27,298 |
| Numerical only, log-transformed, 0s removed | 5,189 | 5,112 |

### Decision Trees

| Config | RMSE Train | RMSE Test |
|--------|-------------|-----------|
| All features, scaling/logs | 21,707 | 21,859 |
| Numerical only, log-transformed, 0s removed | 4,856 | 5,013 |

ğŸ’¡ The Decision Tree with minimal preprocessing gave the **best result** with the lowest RMSE.

---

## ğŸ“ˆ Insights & Learnings

- Removing 0s from the target and applying logs significantly improved model accuracy
- Simpler models (e.g., Decision Tree without full encoding) can outperform more complex ones
- Proper handling of outliers and missing values is critical in real-world regression problems

---

## ğŸ‘¤ Author

**Aleksander Tanskii**  

---

ğŸ” Interested in financial data, credit risk modeling, or practical regression use cases? Feel free to explore this project and reuse the workflow!
